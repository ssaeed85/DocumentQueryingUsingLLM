{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67059f69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Business Understanding\n",
    "\n",
    "Document parsers are a powerful tool that can help enterprises automate the process of extracting data from documents, leading to significant savings in time and money, and can also help to improve the accuracy and efficiency of business processes. \n",
    "\n",
    "They can be used to extract data from a wide variety of document types, including employee handbooks, catalogs, invoices, purchase orders, sales orders, shipping and delivery orders, form-based contracts, HR and admin documents, bank and credit card statements, fillable PDF forms, and Word documents. \n",
    "\n",
    "LLMs are still under development, but they have the potential to revolutionize the way we interact with documents. They can help us to find information more quickly and easily, understand documents more deeply, and generate new content based on the information in documents. They can be particularly powerful for querying against documents because they can understand the context of the documents and can generate responses that are relevant and informative. \n",
    "\n",
    "Both Google and Microsoft Azure provide a streamlined setup to be able to do this as part of their AI ventures and platforms. As an exercise I wanted to build such a tool on my local machine. \n",
    ">The purpose of this exercise is to try and implement a LLM response to a localized document. \n",
    "\n",
    "In lieu of a personal document that might be parsed against, I'll be using fairytales we are all somewhat familiar with from the fantastic [Project Gutenberg](https://www.gutenberg.org/ebooks/search/?sort_order=downloads)\n",
    "\n",
    "We will try to implement an LLM querying system using natural language prompts to answer using the documents provided. \n",
    "\n",
    "**An Important Note**: Since these documents are in the public domain, the LLM might already be aware of the information, so we will prompt the model to only look at the data provided to the LLM as part of the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e2d67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T03:22:36.959009Z",
     "start_time": "2023-10-18T03:22:36.944264Z"
    }
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a86d6bb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T03:34:17.580057Z",
     "start_time": "2023-10-18T03:34:17.568237Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db0c191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T03:22:36.959009Z",
     "start_time": "2023-10-18T03:22:36.944264Z"
    }
   },
   "source": [
    "# Document Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd4b72-1d1f-4843-be45-dac740801984",
   "metadata": {},
   "source": [
    "We are going to start with loading the necessary text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b68b42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T03:37:14.200452Z",
     "start_time": "2023-10-18T03:37:14.175502Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/AliceInWonderland.txt', 'data/DollsHouse.txt', 'data/Dracula.txt', 'data/Frankenstein.txt', 'data/LettersFromACat.txt', 'data/Metamorphosis.txt', 'data/PictureOfDorianGray.txt', 'data/PrideAndPrejudice.txt', 'data/RomeoAndJuliet.txt', 'data/ScarletLetter.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for (root, folders, files) in os.walk(top = 'data'):\n",
    "    \n",
    "    print([f\"{root}/{file}\"  for file in files])\n",
    "    textFiles = [TextLoader(file_path = f\"{root}/{file}\",autodetect_encoding=True).load() for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5e0756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T03:31:57.587801Z",
     "start_time": "2023-10-18T03:31:57.566026Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/AliceInWonderland.txt'}\n",
      "{'source': 'data/DollsHouse.txt'}\n",
      "{'source': 'data/Dracula.txt'}\n",
      "{'source': 'data/Frankenstein.txt'}\n",
      "{'source': 'data/LettersFromACat.txt'}\n",
      "{'source': 'data/Metamorphosis.txt'}\n",
      "{'source': 'data/PictureOfDorianGray.txt'}\n",
      "{'source': 'data/PrideAndPrejudice.txt'}\n",
      "{'source': 'data/RomeoAndJuliet.txt'}\n",
      "{'source': 'data/ScarletLetter.txt'}\n"
     ]
    }
   ],
   "source": [
    "for textFile in textFiles:\n",
    "    print(textFile[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2fa206-dc40-484b-a81e-0d0a0ffcfd49",
   "metadata": {},
   "source": [
    "The metadata can be better. Instead of having a giant string of the `file path`, I will instead convert it to `file location`, `file type` and `title`. I am most interested in keeping the `title`, the others I'll track primarily for posterity. This can provide one extra datapoint when we are trying to match our queries to the right file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a7a5e5-a675-480e-9534-dc2f9a576fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanMetaData(metadata):\n",
    "    import re\n",
    "    source = metadata['source']\n",
    "    metadataStrSplit = re.split('/|\\.',source)\n",
    "\n",
    "    fileType = metadataStrSplit.pop()\n",
    "    title = metadataStrSplit.pop()\n",
    "    fileLocation = '/'.join(metadataStrSplit)\n",
    "\n",
    "    return {\n",
    "        'file location':fileLocation,\n",
    "        'file type':fileType,\n",
    "        'file title':title\n",
    "    }\n",
    "\n",
    "for textFile in textFiles:\n",
    "    metaDataKeys = textFile[0].metadata.keys()\n",
    "    if 'source' in metaDataKeys:\n",
    "        textFile[0].metadata = cleanMetaData(textFile[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8cb5df-c353-4c4b-bb4f-495e89a48d50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file location': 'data', 'file type': 'txt', 'file title': 'AliceInWonderland'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'DollsHouse'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'Dracula'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'Frankenstein'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'LettersFromACat'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'Metamorphosis'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'PictureOfDorianGray'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'PrideAndPrejudice'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'RomeoAndJuliet'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'ScarletLetter'}\n"
     ]
    }
   ],
   "source": [
    "for textFile in textFiles:\n",
    "    print(textFile[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd254025-8283-49a2-be84-6380218cacbd",
   "metadata": {},
   "source": [
    "# Text Splitter: Chunkify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136bc34f-8083-40e8-a75d-e7e2c799440b",
   "metadata": {},
   "source": [
    "We will be splitting up our documents into smaller chunks. This will serve 2 vital purpose:\n",
    "- LLMs prompts are limited to a certain number of tokens per query. \n",
    "- More importantly, from a cost management standpoint, instead of passing an entire document (in parts or otherwise) becomes cost prohibitive since LLMs charge by the token. Instead of passing the entire document to query against, its in our best interest to come up with a way to pull out only the relevant parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "185afdfb-03e4-4687-949a-1d4a1e2f7b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = ['\\n\\n','\\n', '.',' '],\n",
    "    keep_separator=False,\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014c9c4b-e9df-41a6-a1bb-4c660784a2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5161\n"
     ]
    }
   ],
   "source": [
    "chunks = splitter.split_documents(documents=[textFile[0] for textFile in textFiles])\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c64f8-7447-40ef-b94d-daead99dab91",
   "metadata": {},
   "source": [
    "Lets have a look at one of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e91884c-8d89-42cd-8c18-19e1c8715bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def printChunkInfo(doc, strLen = 300):   \n",
    "    \"\"\"\n",
    "    This function pretty prints a lanchain document by printing the first `strLen` characters of the page content along with the metadata.\n",
    "\n",
    "    Args:\n",
    "    doc: Lanchain document objects.\n",
    "    strLen: The number of characters to print from the page_content string.\n",
    "    \"\"\"\n",
    "    def pretty_print_dict(dict1):\n",
    "        print('{')\n",
    "        for key, value in dict1.items():\n",
    "            print(f'  {key}: {value}')\n",
    "        print('}')\n",
    "    \n",
    "    pagecontent = doc.page_content\n",
    "    metadata    = doc.metadata\n",
    "    \n",
    "    print(f\"Printing chunk (First {strLen} chars and the metadata)\")\n",
    "    print('-'*100)\n",
    "    print(f\"Page Content:\\n{pagecontent[:strLen]}\")\n",
    "    print('-'*100)\n",
    "    print(f\"MetaData:\")\n",
    "    pretty_print_dict(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dff9727-8350-4558-a3ea-309b20b8962a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "properties of native herbs and roots; nor did he conceal from his\n",
      "patients, that these simple medicines, Nature’s boon to the untutored\n",
      "savage, had quite as large a share of his own confidence as the\n",
      "European pharmacopœia, which so many learned doctors had spent\n",
      "centuries in elaborating.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: ScarletLetter\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0,len(chunks))\n",
    "randomchunk = chunks[idx]\n",
    "\n",
    "printChunkInfo(randomchunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a1436-9006-4754-9db5-da3e6cdd35db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Looks like we have our corpus ready which is a list of chunks made from all the documents in our data folder. Now we can pass each and every `chunk` to the `LLM` model and have it answer the question using each chunk as a source. \n",
    "\n",
    "However, he current approach of passing each and every chunk to the `LLM` model has a number of drawbacks. Firstly, it is computationally expensive, as the model has to process a large amount of data. Secondly, it can be time-consuming, as the model has to make a large number of calls to the `LLM` API. Thirdly, it can be inaccurate, as the model may be misled by irrelevant chunks. \n",
    "\n",
    "A more advantageous approach would be to pass only relevant chunks to the LLM model. This would reduce the number of calls to the LLM API, as well as the amount of data that the model has to process, which should lead to a more efficient and cost-effective approach. Additionally, it would improve the accuracy of the model, as the model would not be misled by irrelevant chunks. We can then use the LLM to summarize and give us a final answer based on the answers that it gave for the core *n* chunks.\n",
    "\n",
    "There are a number of ways to find relevant chunks within a corpus. \n",
    "\n",
    "- We could have a `human-in-the-loop` system. In this system, a human would identify the relevant chunks. The identified chunks would then be passed to the LLM model.This is normally the least time efficient solution, even for a marignal sized corpus\n",
    "- One common automated approach is to use `keyword search`. This involves searching the corpus for chunks that contain specific keywords or phrases. For example, if we are interested in finding chunks about the topic of \"natural language processing,\" we could search the corpus for chunks that contain the keywords \"natural language processing,\" \"NLP,\" or \"machine translation.\"\n",
    "- The more modern approach would be to use `embeddings similarity` measures to find relevant chunks within a corpus. Embeddings similarity measures allow us to measure the similarity between two chunks by comparing their embedding vectors. For example, we could use the `cosine similarity` measure to compare the embedding vectors of two words \"king\" and \"emperor\". The cosine similarity measure would return a value between 0 and 1, where a higher value indicates a greater similarity between the two chunks. We can also use `L2` distance as a measure (`euclidean distance`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1b97c-b25b-47be-90c5-6d2747cc32c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "To use embeddings similarity measures to find relevant chunks within a corpus, we first need to generate embeddings for all of the chunks in the corpus. This can be done using a pre-trained embedding model, such as `Word2Vec`, `GloVe`, `BERT` etc. \n",
    "\n",
    "Once we have generated embeddings for all of the chunks, we can then use a similarity measure to compare the embedding vectors of any chunk to the remaining corpus. More directly, we can generate the embeddings for the question itself and measure its similarity of to our copus of chunks. The chunks with the highest similarity scores are the most relevant to the question prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87c87a-6223-481b-934e-2457c24cbe8f",
   "metadata": {},
   "source": [
    "## Hugging Face  🤗 \n",
    "\n",
    "The latest in the field of embeddings and sentence similarity matching are models such as `BERT` and there are new ones that are popping up every day.  🤗  Transformers library provides a number of pre-trained tokenizer models, including `BERT`, `RoBERTa`, `DistilBERT`, `ALBERT`, `XLNet`, and `T5` which have been trained on large datasets to help perform NLP tasks more accurately.\n",
    "\n",
    "All of these models return embeddings, or vectors, of specified dimensions for every `token` it processes. In essence for every chunk multiple embeddings would be generated. Since we are more interested in the similarity between `chunk`s and not `token`s, these embeddings would then have combined using techniques like `mean pooling` or `max pooling`, accounting for `padding`, similar to computer vision problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c236d68-fe6a-432e-bb57-b00665d55989",
   "metadata": {},
   "source": [
    "## Sentence Transformers\n",
    "However, the easier way would be to leverage the `sentence-transformers` framework on  🤗 , which would would do a lot of the above work for us. Looking at the top downloads on the `sentence-transformers` page, it seems like the most popular model is [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)\n",
    "\n",
    "![title](img/TopSentenceTransformers.png)\n",
    "\n",
    "\n",
    "To help speed up the process I am going to place the model on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e1f235-b1bd-4305-aa25-fb62ece9af03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "STmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "STmodel.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23a061-f9ff-4854-a0aa-31e7e16d4ab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Corpus Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a355afae-bc7c-4107-87ad-8c3096b1591a",
   "metadata": {},
   "source": [
    "### Test Case\n",
    "Our corpus is a list `langchain` Document objects. Each document has 2 parts:\n",
    "- page_content: the text section of the chunk\n",
    "- metadata: citation\n",
    "\n",
    "\n",
    "We are primarily going to rely on the `page_content` section of a document. However, the metadata section can be useful to keep around for citations, and can add another data point to use. Lets append the texts together and test on the same random chunk as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a0af9dd-33bf-4e1c-ac06-719a899e2e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"properties of native herbs and roots; nor did he conceal from his\\npatients, that these simple medicines, Nature’s boon to the untutored\\nsavage, had quite as large a share of his own confidence as the\\nEuropean pharmacopœia, which so many learned doctors had spent\\ncenturies in elaborating. \\nMedata: {'file location': 'data', 'file type': 'txt', 'file title': 'ScarletLetter'}\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createChunkDocInfo(chunk):\n",
    "    return chunk.page_content +' \\nMedata: '+ str(chunk.metadata)\n",
    "\n",
    "createChunkDocInfo(randomchunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35e64d1-6b1c-4cf6-81d6-cbd68f02a266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getEmbeddings(model, listOfText):\n",
    "    return model.encode(listOfText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bac834cd-dead-4b9c-858b-bcf93ed897db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.41518097e-02  6.31735800e-03 -1.86745618e-02  1.32730289e-03\n",
      " -1.93464737e-02 -8.53492506e-03  1.81569457e-02  4.65971120e-02\n",
      "  5.47228009e-02 -2.94660330e-02  2.86817476e-02  7.62453973e-02\n",
      " -2.66168229e-02 -1.84038728e-02  2.63257753e-02  3.93042043e-02\n",
      "  6.57500420e-03  3.22241634e-02 -2.11457703e-02 -7.37931440e-03\n",
      " -3.03081833e-02  1.98881272e-02 -1.80783086e-02  2.06939820e-02\n",
      "  2.21601687e-02 -3.55543904e-02  2.70343088e-02  7.09343515e-03\n",
      "  1.31718647e-02 -9.72086266e-02 -2.22905707e-02 -4.87651350e-03\n",
      " -5.18808551e-02 -3.87605652e-02  1.92325638e-06 -1.38447555e-02\n",
      "  2.06677262e-02  1.86542571e-02 -2.85016503e-02 -1.70920044e-02\n",
      "  7.33358189e-02 -1.05369575e-02 -1.67293870e-03 -8.41918681e-03\n",
      " -4.46147732e-02 -4.67648543e-02 -4.58565764e-02  1.92460287e-02\n",
      " -6.01361841e-02  3.44090350e-02  1.18462071e-02 -8.11253302e-03\n",
      " -1.67945642e-02 -3.49614099e-02  1.10484801e-01  1.14914065e-03\n",
      "  6.07331097e-03 -4.56958227e-02  5.64676821e-02  2.05282979e-02\n",
      " -1.64752193e-02 -6.27124915e-03 -4.19888683e-02 -2.74174195e-02\n",
      " -2.37602368e-02 -7.13651301e-03  8.59581307e-03 -5.35556078e-02\n",
      "  9.46269743e-03 -8.80131684e-03  4.08487488e-03 -1.32575340e-03\n",
      "  1.65510792e-02  7.31341094e-02 -7.35737979e-02 -4.09690812e-02\n",
      " -1.93150416e-02 -1.69706456e-02 -2.72728968e-02 -2.87484583e-02\n",
      "  5.37214242e-02  2.48387586e-02  3.54828164e-02  1.50072649e-02\n",
      "  2.39581633e-02  3.53196748e-02  1.40641592e-02  5.61878607e-02\n",
      " -1.10651110e-03 -8.16242322e-02  2.78012175e-02 -4.81967442e-02\n",
      "  4.37635258e-02  3.60907502e-02  8.17278121e-03 -5.82159217e-03\n",
      "  6.71226755e-02 -5.11368699e-02  7.09796920e-02  1.95302404e-02]\n"
     ]
    }
   ],
   "source": [
    "embeddings = getEmbeddings(STmodel, createChunkDocInfo(randomchunk))\n",
    "print(embeddings[:100]) #Only creating the first 100 values of the embeddings vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25f2bccf-bfab-4441-9d3d-809177ad5a83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing length of first element of embeddings\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6baa1-d17c-43c0-ad2e-59a77ef506cb",
   "metadata": {},
   "source": [
    "Each embedding is a vector that is 768 elements long. The first 100 elements look like the print out above.\\\n",
    "Now to create embeddings for our entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1811b7c3-02cc-4b0e-a325-b7cb84dc2f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating a list of strings using the chunks of lanchain documents\n",
    "sentences = [createChunkDocInfo(chunk) for chunk in chunks]\n",
    "\n",
    "#creating embeddings for the resulting sentences\n",
    "embeddings = getEmbeddings(STmodel,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6044790-cc6e-45f5-9828-e56c6542d639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5161, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7514b8-fbb3-4ede-93b9-313a38383ce8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf4fe7-3616-4870-92f1-5eb6eab359ca",
   "metadata": {},
   "source": [
    "> [FAISS](https://ai.meta.com/tools/faiss/#:~:text=FAISS%20(Facebook%20AI%20Similarity%20Search,more%20scalable%20similarity%20search%20functions) (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves limitations of traditional query search engines that are optimized for hash-based searches, and provides more scalable similarity search functions.\\\n",
    "> *-- FAISS page*\n",
    "\n",
    "`FAISS` effectively can act like a local vector DB to help store all our embeddings and efficiently retrieve similar embeddings and consequently, semantically similar sentences\n",
    "\n",
    "An example on how to implement FAISS using  🤗  transformers can be found [here](https://huggingface.co/learn/nlp-course/chapter5/6?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d2e4223-27be-41dc-ae34-e1e4ee40236c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "from faiss import write_index, read_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ca367-90cd-448d-9663-62644d54f6cf",
   "metadata": {},
   "source": [
    "If you're running the indexing the first time, the `FAISS` model will have to be run and depending on how large the corpus is and how it has been chunked, it can take a while. However, on future runs, the index can be directly loaded from your local hard drive if we write it to our local machine.\n",
    "\n",
    "The following cell tries to load the `FAISS` index from your local hard drive, and failing to do so, runs the indexing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7b9bedb-6b2b-4ad0-b226-98065f916332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index does not exist on local hard drive\n",
      "Creating FAISS index\n",
      "Writing index to local machine.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    index = read_index(\"faiss_index/FAISS_Embeddings.index\")\n",
    "    print(\"FAISS index successfully loaded from local machine\")\n",
    "except:\n",
    "    res = faiss.StandardGpuResources()\n",
    "    \n",
    "    print(\"FAISS index does not exist on local hard drive\")    \n",
    "    print(\"Creating FAISS index\")\n",
    "    \n",
    "    # build a flat (CPU) index\n",
    "    index_flat = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    # make it into a gpu index\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "    \n",
    "    gpu_index_flat.add(embeddings)         # add vectors to the index\n",
    "\n",
    "    print(\"Writing index to local machine.\")\n",
    "    write_index(index_flat, \"faiss_index/large.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605adf2b-03f8-49aa-8799-88c07635fad3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sample Query\n",
    "We'll use the same function as before to generate embeddings for potential queries. Transformer models are optimized to work well with vectors, and our function leverages that. As a result we can encode multiple queries simultaneously, and even if we have a singular query we should enclose it in a list.\n",
    "\n",
    "Lets try to do a semantic search on a few sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "485c01d6-6190-46c9-8b70-44738b3ea302",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\"where does the story of frakenstein take place?\",\"where does Alice in Wonderland take place?\"]\n",
    "queriesEmb = getEmbeddings(STmodel,queries)\n",
    "\n",
    "queriesEmb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77239bab-05de-4794-a571-120d611cdd02",
   "metadata": {},
   "source": [
    "The search function in `FAISS` is a generic function that can be used to search for the nearest neighbors of a query vector in a given index. The function takes the following arguments:\n",
    "- query: The query vector.\n",
    "- k: The number of nearest neighbors to return.\n",
    "\n",
    "and returns:\n",
    "- distances: A pre-allocated buffer to store the distances to the nearest neighbors.\n",
    "- indexes: A pre-allocated buffer to store the labels of the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60ecb40d-6d80-4caf-9f56-2b38b2087748",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Distance of neghbors to query:\n",
      " [[1.0184469  1.0542233  1.082648   1.0858334  1.090111  ]\n",
      " [0.67878747 0.7000439  0.81062174 0.9521235  0.96657526]]\n",
      "--------------------------------------------------\n",
      "Index of neghbors:\n",
      "[[1659 1836 1990 1604 1985]\n",
      " [ 179    0    1  178  107]]\n",
      "--------------------------------------------------\n",
      "Note: There were 2 queries\n",
      "['where does the story of frakenstein take place?', 'where does Alice in Wonderland take place?']\n"
     ]
    }
   ],
   "source": [
    "numNeighbors = 5                          \n",
    "Distances, Indexes = gpu_index_flat.search(x=queriesEmb, k=numNeighbors)  \n",
    "\n",
    "print('-'*50)\n",
    "print(f\"Distance of neghbors to query:\\n {Distances[:5]}\")      \n",
    "print('-'*50)             \n",
    "print(f\"Index of neghbors:\\n{Indexes[:5]}\")                   \n",
    " \n",
    "print('-'*50)\n",
    "print('Note: There were 2 queries')\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eddd760-9058-4f27-9bb7-d7454e121369",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "properties of native herbs and roots; nor did he conceal from his\n",
      "patients, that these simple medicines, Nature’s boon to the untutored\n",
      "savage, had quite as large a share of his own confidence as the\n",
      "European pharmacopœia, which so many learned doctors had spent\n",
      "centuries in elaborating.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: ScarletLetter\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "c = chunks[idx]\n",
    "printChunkInfo(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "567f2fae-d098-424a-ad69-3342da01a112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "For the query:    'where does the story of frakenstein take place?':\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "Chunk Index: 1659\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "I am already far north of London, and as I walk in the streets of\n",
      "Petersburgh, I feel a cold northern breeze play upon my cheeks, which\n",
      "braces my nerves and fills me with delight. Do you understand this\n",
      "feeling? This breeze, which has travelled from the regions towards\n",
      "which I am advancing, gives me\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: Frankenstein\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 1836\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "I quitted my seat, and walked on, although the darkness and storm\n",
      "increased every minute, and the thunder burst with a terrific crash\n",
      "over my head. It was echoed from Salêve, the Juras, and the Alps of\n",
      "Savoy; vivid flashes of lightning dazzled my eyes, illuminating the\n",
      "lake, making it appear like a \n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: Frankenstein\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 1990\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "“She arrived in safety at a town about twenty leagues from the cottage\n",
      "of De Lacey, when her attendant fell dangerously ill. Safie nursed her\n",
      "with the most devoted affection, but the poor girl died, and the\n",
      "Arabian was left alone, unacquainted with the language of the country\n",
      "and utterly ignorant of\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: Frankenstein\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 1604\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "_6 November._--It was late in the afternoon when the Professor and I\n",
      "took our way towards the east whence I knew Jonathan was coming. We did\n",
      "not go fast, though the way was steeply downhill, for we had to take\n",
      "heavy rugs and wraps with us; we dared not face the possibility of being\n",
      "left without warm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: Dracula\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 1985\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "“The government of France were greatly enraged at the escape of their\n",
      "victim and spared no pains to detect and punish his deliverer. The\n",
      "plot of Felix was quickly discovered, and De Lacey and Agatha were\n",
      "thrown into prison. The news reached Felix and roused him from his\n",
      "dream of pleasure. His blind \n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: Frankenstein\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "For the query:    'where does Alice in Wonderland take place?':\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "Chunk Index: 179\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "Lastly, she pictured to herself how this same little sister of hers\n",
      "would, in the after-time, be herself a grown woman; and how she would\n",
      "keep, through all her riper years, the simple and loving heart of her\n",
      "childhood: and how she would gather about her other little children,\n",
      "and make _their_ eyes b\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: AliceInWonderland\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 0\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "The Project Gutenberg eBook of Alice's Adventures in Wonderland\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gu\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: AliceInWonderland\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 1\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "Alice’s Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Contents\n",
      "\n",
      " CHAPTER I.     Down the Rabbit-Hole\n",
      " CHAPTER II.    The Pool of Tears\n",
      " CHAPTER III.   A Caucus-Race and a Long Tale\n",
      " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
      " CHAPTER V.     Advice from a Cater\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: AliceInWonderland\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 178\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "So she sat on, with closed eyes, and half believed herself in\n",
      "Wonderland, though she knew she had but to open them again, and all\n",
      "would change to dull reality—the grass would be only rustling in the\n",
      "wind, and the pool rippling to the waving of the reeds—the rattling\n",
      "teacups would change to tinkling \n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: AliceInWonderland\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 107\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "Once more she found herself in the long hall, and close to the little\n",
      "glass table. “Now, I’ll manage better this time,” she said to herself,\n",
      "and began by taking the little golden key, and unlocking the door that\n",
      "led into the garden. Then she went to work nibbling at the mushroom\n",
      "(she had kept a piec\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: AliceInWonderland\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for q_num,q_str in enumerate(queries):\n",
    "    print('*'*100)\n",
    "    print(f\"For the query:    '{q_str}':\")\n",
    "    print('*'*100)\n",
    "    print('\\n')\n",
    "    for idx in Indexes[:5][q_num]:\n",
    "        print(f\"Chunk Index: {idx}\")\n",
    "        printChunkInfo(chunks[idx])\n",
    "        print('\\n')\n",
    "        print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a0175-c0cb-47c3-b746-f2355a48382e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Looks like our semantic search is working reasonably well. Although clearly some errors are being made. For eg:\n",
    "- The first query was in regards to `Frankenstein`'s story, instead it pulled information from `Dracula`. \n",
    "- For the second query, although all queries are from `Alice in Wonderland` per the query request, the top chunk has more to do with metadata about the printing of the book that was part of the text file.\n",
    "\n",
    "The model is probably relying fairly heavily on the tile in the metadata. If the filenames were completely random, or named using some kind of hashing, it may struggle to find relevant chunks. \n",
    "\n",
    "We'll see whether the LLM is able to distinguish the relevance, between the chunks to see whether it realizes what are passages from the story and what isn't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0088e1-7157-43a5-a02f-f04b1cea0812",
   "metadata": {},
   "source": [
    "# LLM  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294f7d7-219b-4e25-9d55-92d21babca7b",
   "metadata": {},
   "source": [
    "I am going to use `OpenAI`s LLM models, but even there you have a few differen't choices. `Da-Vinci` can help keep your costs down. I am going to opt for the ChatGPT 3.5 model because it is more performant. One can even \n",
    "\n",
    "For my purposes, I am going to pass a query and a list of relevant docs for the model to create responses. For this step I'd rather have a more precise and succint model, while utitlizing a more descriptive and creative model to summarize the final answer. So even though I have the same end point (`gpt-3.5-turbo`), I create 2 seperate rulesets for the model.\n",
    "\n",
    "Both of them are instructed to answer based on only the information provided.\n",
    "\n",
    "Note: Until this step, all work has been performed on the local hardware, and no costs were incurred. API calls to the `OpenAI` will result in cost based on the number of tokens passed as part of the API call. A valid API key will also be necessary which can be created at the [OpenAI platform website](https://platform.openai.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f45c35f4-7bf3-49cc-a805-759187944cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2693e1a-259f-45d2-9d81-d460978ae1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load API Key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ['API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7852272e-4490-4b51-a4e5-d8e39cb5035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm_precise = ChatOpenAI(\n",
    "    temperature=0.0, \n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "chatllm_descriptive = ChatOpenAI(\n",
    "    temperature=0.7, \n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "\n",
    "def queryLLM(query, \n",
    "             excerpt, \n",
    "             llm=chatllm_precise):    \n",
    "    \"\"\"\n",
    "    Queries a large language model (LLM) to answer a question using an excerpt of text.\n",
    "\n",
    "    Args:\n",
    "    query: The question to be answered.\n",
    "    excerpt: The excerpt of text to use to answer the question.\n",
    "        llm: The ChatOpenAI object to use.\n",
    "\n",
    "    Returns:\n",
    "    The LLM's response to the query.\n",
    "    \"\"\"\n",
    "    assistantRules = (\n",
    "        \"Answer the question using only the excerpt provided. \"+\n",
    "        \"Do not make up information. \"+\n",
    "        \"Be precise and succint. \"+\n",
    "        \"Do not include the question. \"+\n",
    "        \"Do not include an introduction in your response. \"+\n",
    "        \"Do not mention the excerpt.\"\n",
    "    )\n",
    "    \n",
    "    msg = f\"Q: {query}\" + \\\n",
    "          f\" Excerpt: {excerpt}\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(role = 'assistant',\n",
    "                      content=assistantRules), \n",
    "        HumanMessage(content=msg)\n",
    "    ]\n",
    "    \n",
    "    return llm(messages)\n",
    "\n",
    "def summarizeReponsesLLM(query, \n",
    "                         responses, \n",
    "                         llm=chatllm_descriptive):\n",
    "    \"\"\"\n",
    "    Summarizes the given responses using the given LLM model.\n",
    "\n",
    "    Args:\n",
    "        query: The query that was asked.\n",
    "        responses: A list of response objects.\n",
    "        llm: The ChatOpenAI object to use.\n",
    "\n",
    "    Returns:\n",
    "        A summary of the given responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    concat_responses = ' '.join([response.content for response in responses])\n",
    "    \n",
    "    assistantRules = (\n",
    "        \"Answer the question by summarizing the responses. \"+\n",
    "        \"Do not make up information.\"+\n",
    "        \"Do not include the question. \"+\n",
    "        \"Do not include an introduction in your response. \" +\n",
    "        \"Do not mention excerpts that provide no information\"\n",
    "    ) \n",
    "    \n",
    "    \n",
    "    msg = f\"Q: {query}\" + \\\n",
    "          f\" Responses: {concat_responses}\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(role = 'assistant',\n",
    "                      content=assistantRules), \n",
    "        HumanMessage(content=msg)\n",
    "    ]\n",
    "    \n",
    "    return llm(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856bd74-096f-4f8c-9ecb-2198a2f3e938",
   "metadata": {},
   "source": [
    "### Test API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82e5c75e-b78d-4b00-8902-a4f3a1cd4877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The temperature today in Dallas is expected to be mainly in the 90s.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryLLM(query = \"whats the temperature today in Dallas?\",\n",
    "         excerpt= \"A warm weekend is expected. Highs today will be mainly in the 90s with 80s on Sunday. \\\n",
    "         A pattern shift will occur next week resulting in slightly cooler high temps along with scattered showers and thunderstorms each day. Severe weather is not expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b180f1e4-9904-44d2-864f-e2df39b3876a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The excerpt does not provide any information about the temperature in Dallas.')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryLLM(query = \"whats the temperature today in Dallas?\",\n",
    "         excerpt= \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d55c33c-2d1c-4922-9f6d-7826e0715ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The excerpt does not provide information about the temperature in Dallas today.')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryLLM(query = \"whats the temperature today in Dallas?\",\n",
    "         excerpt= \"Dallas (/ˈdæləs/) is a city in Texas and the most populous in the Dallas–Fort Worth metroplex, the fourth-largest metropolitan area in the United States at 7.5 million people.\\\n",
    "         It is the most populous city in and seat of Dallas County with portions extending into Collin, Denton, Kaufman, and Rockwall counties.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af8e67-f64e-46fb-9165-3a1f4de56140",
   "metadata": {},
   "source": [
    "Ok. Looks like our API call functions are working as intended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1345f87-88db-4dc5-8d36-6613c51ef6fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f99ecf29-4e40-40c6-b4df-02299cf3cc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where does the story of frakenstein take place?',\n",
       " 'where does Alice in Wonderland take place?']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cd2f0e6-6319-42c6-952f-23417aa8476a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extractRelevantChunksByIndex(chunks, indexes):\n",
    "    extracted_chunks = []\n",
    "    for index in indexes:\n",
    "        chunkInfo = createChunkDocInfo(chunks[index])\n",
    "        extracted_chunks.append(chunkInfo)\n",
    "\n",
    "    return extracted_chunks\n",
    "\n",
    "\n",
    "def getMostRelevantChunks_perQuery( queries, \n",
    "                                    numNeighbors, \n",
    "                                    chunks,\n",
    "                                    faiss_index = gpu_index_flat,\n",
    "                                    sentenceTransformer = STmodel):\n",
    "    \n",
    "    queriesEmb = getEmbeddings(sentenceTransformer,queries)    \n",
    "    Distances, mostRelevantChunkIndexes_perQuery = faiss_index.search(queriesEmb,\n",
    "                                                                k = numNeighbors) \n",
    "        \n",
    "    mostRelevantChunks_perQuery = [extractRelevantChunksByIndex(chunks, indexes)\n",
    "                                   for indexes in mostRelevantChunkIndexes_perQuery]\n",
    "        \n",
    "    return queries, queriesEmb, mostRelevantChunks_perQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ede8f38f-af52-49e5-b7c7-147ac99f9b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def queryAndSummarize(queries, \n",
    "                      chunks = chunks,\n",
    "                      numNeighbors=5):\n",
    "    responses_perQuery = defaultdict(list)\n",
    "    Results = {}\n",
    "    \n",
    "    queries, queriesEmb, relevantChunks_perQuery = getMostRelevantChunks_perQuery(queries,\n",
    "                                                                                 numNeighbors = numNeighbors,\n",
    "                                                                                 chunks = chunks)\n",
    "\n",
    "    for query, relevantChunks in zip(queries, relevantChunks_perQuery):\n",
    "        for chunk in relevantChunks:\n",
    "            response = queryLLM(query, chunk)\n",
    "            responses_perQuery[query].append(response)\n",
    "\n",
    "        finalAnswer = summarizeReponsesLLM(query=query,\n",
    "                                        responses=responses_perQuery[query]).content\n",
    "\n",
    "        Results[query] = {\n",
    "            'Responses': responses_perQuery[query],\n",
    "            'Summary':   finalAnswer\n",
    "        }\n",
    "        \n",
    "    return Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44287662-576a-4387-a3d8-896c187bd808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = queryAndSummarize(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e42f4b34-9934-48b0-8a70-feeb904f5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprintResponses(responses):\n",
    "    for k,v in responses.items():\n",
    "        print(f\"Q: {k}\")\n",
    "        print(f\"A: {v['Summary']}\")\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a137f30b-9f7d-43a3-945e-5db0dcc077da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: where does the story of frakenstein take place?\n",
      "A: The story of Frankenstein takes place in Petersburgh, which is located far north of London. It also takes place in Switzerland, specifically near the town, lake, and mountains mentioned in the excerpt. It also takes place in a town about twenty leagues from the cottage of De Lacey. The story does not take place in the provided excerpt from \"Dracula\" in the Carpathian mountains. The story of Frankenstein does not take place in France and Italy.\n",
      "\n",
      "\n",
      "Q: where does Alice in Wonderland take place?\n",
      "A: Alice in Wonderland takes place in Wonderland, specifically in a beautiful garden with bright flower-beds and cool fountains.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pprintResponses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b978a-d939-428d-b4b9-c74ff844684c",
   "metadata": {},
   "source": [
    "Lets try it with different questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cf45028-aefc-4edf-a2ae-0180a2f35b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which families did Romeo and Juliet belong to?\n",
      "A: Romeo belonged to the Montague family and Juliet belonged to the Capulet family.\n",
      "\n",
      "\n",
      "Q: Who were some of the victims of Dracula?\n",
      "A: The responses do not provide any information about the victims of Dracula.\n",
      "\n",
      "\n",
      "Q: Who were Alice's friends in Alice in Wonderland?\n",
      "A: Alice's friends in Alice in Wonderland are the ten soldiers, the ten courtiers, the ten royal children, the guests (including the White Rabbit), and the Knave of Hearts.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries=[\n",
    "    \"Which families did Romeo and Juliet belong to?\",\n",
    "    \"Who were some of the victims of Dracula?\",\n",
    "    \"Who were Alice's friends in Alice in Wonderland?\"\n",
    "]\n",
    "\n",
    "\n",
    "responses = queryAndSummarize(queries,numNeighbors=10)\n",
    "\n",
    "pprintResponses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67215268-eafc-4d2b-a72a-16ca5808e57f",
   "metadata": {},
   "source": [
    "Although this results in reasonably good answers, but it clearly has limitations. \n",
    "\n",
    "Our embedding semantic search may not be able to help identify who Alice's friends are, the Cheshire Cat and the Mad Hatter don't make an appearance, but the Knave of Hearts who is an enemy does.\n",
    "\n",
    "Similarly, our Dracula victims couldn't be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5da0cca0-4bdf-4c9d-b463-bf8a2538928d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Describe the castle of Dracula.\n",
      "A: The castle of Dracula is located in the extreme east of Transylvania, near the borders of Moldavia and Bukovina. It is situated in the Carpathian mountains, offering a magnificent view of green treetops, deep rifts, and rivers winding through forests. The castle is described as old and big, with broken walls, many shadows, and cold wind blowing through the broken battlements and casements. It has doors everywhere, all of which are locked and bolted, making it feel like a prison. The castle is located on the summit of a sheer precipice, a thousand feet high. The exact location of the castle is not known, but the post town named Bistritz is mentioned. The castle is described as standing high above a wasteland of desolation.\n",
      "\n",
      "\n",
      "Q: Why did Dorian Gray want his portrait made?\n",
      "A: Dorian Gray wanted his portrait made because he was curious about the reason why Basil Hallward had refused to exhibit it. He also believed that the portrait would absorb his whole nature, soul, and art. Additionally, Dorian wanted to hide the portrait to avoid any risk of his friends discovering a secret he had.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries=[\n",
    "    \"Describe the castle of Dracula.\",\n",
    "    \"Why did Dorian Gray want his portrait made?\",\n",
    "]\n",
    "\n",
    "\n",
    "responses = queryAndSummarize(queries,numNeighbors=10)\n",
    "\n",
    "pprintResponses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c6cee1-7024-4685-8098-7c44349bc6ae",
   "metadata": {},
   "source": [
    "# Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30516173-964b-4901-a234-2d2dad3b02a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "When a set of documents is employed, focusing on a specific topic (homogenous documents), can be advantageous. Specifically, it can enable the language model to gain a deeper understanding of the subject matter, including its nuances and specialized terminology. As a result, the model becomes better equipped and theoretically should generate precise and contextually relevant responses when querying these documents.\n",
    "\n",
    "Conversely, when the approach involves a diverse range of documents encompassing various topics, the queries may yield less accurate responses. This is due to the model's understanding being dispersed across multiple subjects, limiting its capacity to provide in-depth and contextually relevant answers.\n",
    "\n",
    "Additionally, abstract or loosely defined queries may present challenges, as the model's reliance on concrete knowledge from its training data may hinder its ability to provide accurate responses. \n",
    "\n",
    "Thus, the specificity and quality of the documents utilized for querying significantly influence the success of the information retrieval process and are **paramount**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4de4a-27aa-45f0-9199-a62c5eda621d",
   "metadata": {},
   "source": [
    "# References and Citations:\n",
    "- [Project Gutenberg](https://www.gutenberg.org/ebooks/search/?sort_order=downloads)\n",
    "- [Understanding Neural Network Embeddings](https://towardsdatascience.com/understanding-neural-network-embeddings-851e94bc53d2) - Frank Liu\n",
    "- [FAISS](https://github.com/facebookresearch/faiss/)\n",
    "- [Hugging Face](https://huggingface.co/)\n",
    "    - [Sentence Transformers](https://huggingface.co/sentence-transformers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

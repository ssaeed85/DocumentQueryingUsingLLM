{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67059f69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Business Understanding\n",
    "\n",
    "Document parsers are a powerful tool that can help enterprises automate the process of extracting data from documents. This can save a significant amount of time and money, and can also help to improve the accuracy and efficiency of business processes.\n",
    "\n",
    "Document parsers can be used to extract data from a wide variety of document types, including employee handbooks, catalogs, invoices, purchase orders, sales orders, shipping and delivery orders, form-based contracts, HR and admin documents, bank and credit card statements, fillable PDF forms, and Word documents.\n",
    "\n",
    "LLMs are particularly powerful for querying against documents because they can understand the context of the documents and can generate responses that are relevant and informative. For example, if you ask an LLM to summarize the main points of a document, it will be able to identify the most important information in the document and present it in a concise and easy-to-understand way.\n",
    "\n",
    "LLMs are still under development, but they have the potential to revolutionize the way we interact with documents. LLMs can help us to find information more quickly and easily, understand documents more deeply, and generate new content based on the information in documents.\n",
    "\n",
    "The purpose of this exercise is to try and implement a LLM response to a localized document. In lieu of a personal document that might be parsed against, I have pulled in some open source documents from the fantastic \n",
    "> https://www.gutenberg.org/ebooks/search/?sort_order=downloads\n",
    "\n",
    "We will try to implement an LLM querying system using natural language prompts to answer using the documents provided. \n",
    "\n",
    "**An Important Note**: Since these are open source documents, the LLM might already be aware of the information, so we will prompt the model to only look at the data provided to the LLM as part of the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e2d67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T03:22:36.959009Z",
     "start_time": "2023-10-18T03:22:36.944264Z"
    }
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86d6bb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T03:34:17.580057Z",
     "start_time": "2023-10-18T03:34:17.568237Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db0c191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T03:22:36.959009Z",
     "start_time": "2023-10-18T03:22:36.944264Z"
    }
   },
   "source": [
    "# Document Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27b68b42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T03:37:14.200452Z",
     "start_time": "2023-10-18T03:37:14.175502Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/AliceInWonderland.txt', 'data/DollsHouse.txt', 'data/Dracula.txt', 'data/Frankenstein.txt', 'data/LettersFromACat.txt', 'data/Metamorphosis.txt', 'data/PictureOfDorianGray.txt', 'data/PrideAndPrejudice.txt', 'data/RomeoAndJuliet.txt', 'data/ScarletLetter.txt']\n"
     ]
    }
   ],
   "source": [
    "for (root, folders, files) in os.walk(top = 'data'):\n",
    "    \n",
    "    print([f\"{root}/{file}\"  for file in files])\n",
    "    textFiles = [TextLoader(file_path = f\"{root}/{file}\",autodetect_encoding=True).load() for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a5e0756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T03:31:57.587801Z",
     "start_time": "2023-10-18T03:31:57.566026Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/AliceInWonderland.txt'}\n",
      "{'source': 'data/DollsHouse.txt'}\n",
      "{'source': 'data/Dracula.txt'}\n",
      "{'source': 'data/Frankenstein.txt'}\n",
      "{'source': 'data/LettersFromACat.txt'}\n",
      "{'source': 'data/Metamorphosis.txt'}\n",
      "{'source': 'data/PictureOfDorianGray.txt'}\n",
      "{'source': 'data/PrideAndPrejudice.txt'}\n",
      "{'source': 'data/RomeoAndJuliet.txt'}\n",
      "{'source': 'data/ScarletLetter.txt'}\n"
     ]
    }
   ],
   "source": [
    "for textFile in textFiles:\n",
    "    print(textFile[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2fa206-dc40-484b-a81e-0d0a0ffcfd49",
   "metadata": {},
   "source": [
    "The metadata can be better. Instead of having a giant string of the `file path`, I will instead convert it to `file location`, `file type` and `title`. I am most interested in keeping the `title`, the others I'll track primarily for posterity. This can provide one extra datapoint when we are trying to match our queries to the right file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39a7a5e5-a675-480e-9534-dc2f9a576fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanMetaData(metadata):\n",
    "    import re\n",
    "    source = metadata['source']\n",
    "    metadataStrSplit = re.split('/|\\.',source)\n",
    "\n",
    "    fileType = metadataStrSplit.pop()\n",
    "    title = metadataStrSplit.pop()\n",
    "    fileLocation = '/'.join(metadataStrSplit)\n",
    "\n",
    "    return {\n",
    "        'file location':fileLocation,\n",
    "        'file type':fileType,\n",
    "        'file title':title\n",
    "    }\n",
    "\n",
    "for textFile in textFiles:\n",
    "    metaDataKeys = textFile[0].metadata.keys()\n",
    "    if 'source' in metaDataKeys:\n",
    "        textFile[0].metadata = cleanMetaData(textFile[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a8cb5df-c353-4c4b-bb4f-495e89a48d50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file location': 'data', 'file type': 'txt', 'file title': 'AliceInWonderland'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'DollsHouse'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'Dracula'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'Frankenstein'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'LettersFromACat'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'Metamorphosis'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'PictureOfDorianGray'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'PrideAndPrejudice'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'RomeoAndJuliet'}\n",
      "{'file location': 'data', 'file type': 'txt', 'file title': 'ScarletLetter'}\n"
     ]
    }
   ],
   "source": [
    "for textFile in textFiles:\n",
    "    print(textFile[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd254025-8283-49a2-be84-6380218cacbd",
   "metadata": {},
   "source": [
    "# Text Splitter: Chunkify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "185afdfb-03e4-4687-949a-1d4a1e2f7b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = ['\\n\\n','\\n', '.',' '],\n",
    "    keep_separator=False,\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "014c9c4b-e9df-41a6-a1bb-4c660784a2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5161\n"
     ]
    }
   ],
   "source": [
    "chunks = splitter.split_documents(documents=[textFile[0] for textFile in textFiles])\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c64f8-7447-40ef-b94d-daead99dab91",
   "metadata": {},
   "source": [
    "Lets have a look at one of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e91884c-8d89-42cd-8c18-19e1c8715bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def printChunkInfo(doc, strLen = 300):   \n",
    "    \"\"\"\n",
    "    This function pretty prints a lanchain document by printing the first `strLen` characters of the page content along with the metadata.\n",
    "\n",
    "    Args:\n",
    "    doc: Lanchain document objects.\n",
    "    strLen: The number of characters to print from the page_content string.\n",
    "    \"\"\"\n",
    "    def pretty_print_dict(dict1):\n",
    "        print('{')\n",
    "        for key, value in dict1.items():\n",
    "            print(f'  {key}: {value}')\n",
    "        print('}')\n",
    "    \n",
    "    pagecontent = doc.page_content\n",
    "    metadata    = doc.metadata\n",
    "    \n",
    "    print(f\"Printing chunk (First {strLen} chars and the metadata)\")\n",
    "    print('-'*100)\n",
    "    print(f\"Page Content:\\n{pagecontent[:strLen]}\")\n",
    "    print('-'*100)\n",
    "    print(f\"MetaData:\")\n",
    "    pretty_print_dict(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3dff9727-8350-4558-a3ea-309b20b8962a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "I used my knowledge of this phase of spiritual pathology, and laid down\n",
      "a rule that she should not be present with Lucy or think of her illness\n",
      "more than was absolutely required. She assented readily, so readily that\n",
      "I saw again the hand of Nature fighting for life. Van Helsing and I were\n",
      "shown up t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: Dracula\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0,len(chunks))\n",
    "randomchunk = chunks[idx]\n",
    "\n",
    "printChunkInfo(randomchunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a1436-9006-4754-9db5-da3e6cdd35db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Looks like we have our corpus ready which is a list of chunks made from all the documents in our data folder. Now we can pass each and every `chunk` to the `LLM` model and have it answer the question using each chunk as a source. \n",
    "\n",
    "However, he current approach of passing each and every chunk to the `LLM` model has a number of drawbacks. Firstly, it is computationally expensive, as the model has to process a large amount of data. Secondly, it can be time-consuming, as the model has to make a large number of calls to the `LLM` API. Thirdly, it can be inaccurate, as the model may be misled by irrelevant chunks. \n",
    "\n",
    "A more advantageous approach would be to pass only relevant chunks to the LLM model. This would reduce the number of calls to the LLM API, as well as the amount of data that the model has to process, which should lead to a more efficient and cost-effective approach. Additionally, it would improve the accuracy of the model, as the model would not be misled by irrelevant chunks. We can then use the LLM to summarize and give us a final answer based on the answers that it gave for the core *n* chunks.\n",
    "\n",
    "There are a number of ways to find relevant chunks within a corpus. \n",
    "\n",
    "- We could have a `human-in-the-loop` system. In this system, a human would identify the relevant chunks. The identified chunks would then be passed to the LLM model.This is normally the least time efficient solution, even for a marignal sized corpus\n",
    "- One common automated approach is to use `keyword search`. This involves searching the corpus for chunks that contain specific keywords or phrases. For example, if we are interested in finding chunks about the topic of \"natural language processing,\" we could search the corpus for chunks that contain the keywords \"natural language processing,\" \"NLP,\" or \"machine translation.\"\n",
    "- The more modern approach would be to use `embeddings similarity` measures to find relevant chunks within a corpus. Embeddings similarity measures allow us to measure the similarity between two chunks by comparing their embedding vectors. For example, we could use the `cosine similarity` measure to compare the embedding vectors of two words \"king\" and \"emperor\". The cosine similarity measure would return a value between 0 and 1, where a higher value indicates a greater similarity between the two chunks. We can also use `L2` distance as a measure (`euclidean distance`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1b97c-b25b-47be-90c5-6d2747cc32c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "To use embeddings similarity measures to find relevant chunks within a corpus, we first need to generate embeddings for all of the chunks in the corpus. This can be done using a pre-trained embedding model, such as `Word2Vec`, `GloVe`, `BERT` etc. \n",
    "\n",
    "Once we have generated embeddings for all of the chunks, we can then use a similarity measure to compare the embedding vectors of any chunk to the remaining corpus. More directly, we can generate the embeddings for the question itself and measure its similarity of to our copus of chunks. The chunks with the highest similarity scores are the most relevant to the question prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87c87a-6223-481b-934e-2457c24cbe8f",
   "metadata": {},
   "source": [
    "## Hugging Face  ü§ó \n",
    "\n",
    "The latest in the field of embeddings and sentence similarity matching are models such as `BERT` and there are new ones that are popping up every day.  ü§ó  Transformers library provides a number of pre-trained tokenizer models, including `BERT`, `RoBERTa`, `DistilBERT`, `ALBERT`, `XLNet`, and `T5` which have been trained on large datasets to help perform NLP tasks more accurately.\n",
    "\n",
    "All of these models return embeddings, or vectors, of specified dimensions for every `token` it processes. In essence for every chunk multiple embeddings would be generated. Since we are more interested in the similarity between `chunk`s and not `token`s, these embeddings would then have combined using techniques like `mean pooling` or `max pooling`, accounting for `padding`, similar to computer vision problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c236d68-fe6a-432e-bb57-b00665d55989",
   "metadata": {},
   "source": [
    "## Sentence Transformers\n",
    "However, the easier way would be to leverage the `sentence-transformers` framework on  ü§ó , which would would do a lot of the above work for us. Looking at the top downloads on the `sentence-transformers` page, it seems like the most popular model is [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)\n",
    "\n",
    "![title](img/TopSentenceTransformers.png)\n",
    "\n",
    "\n",
    "To help speed up the process I am going to place the model on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46e1f235-b1bd-4305-aa25-fb62ece9af03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "STmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "STmodel.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23a061-f9ff-4854-a0aa-31e7e16d4ab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Corpus Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a355afae-bc7c-4107-87ad-8c3096b1591a",
   "metadata": {},
   "source": [
    "### Test Case\n",
    "Our corpus is a list `langchain` Document objects. Each document has 2 parts:\n",
    "- page_content: the text section of the chunk\n",
    "- metadata: citation\n",
    "\n",
    "\n",
    "We are primarily going to rely on the `page_content` section of a document. However, the metadata section can be useful to keep around for citations, and can another data point to find a match on. Lets append the texts together and test on the same random chunk as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a0af9dd-33bf-4e1c-ac06-719a899e2e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"was by good luck, for I am sure she did not listen. I was sometimes\\n     quite provoked; but then I recollected my dear Elizabeth and Jane,\\n     and for their sakes had patience with her. Mr. Darcy was punctual\\n     in his return, and, as Lydia imformed you, attended the wedding. He\\n     dined with us the next day, and was to leave town again on\\n     Wednesday or Thursday. Will you be very angry with me, my dear\\n     Lizzy, if I take this opportunity of saying (what I was never bold\\n     enough to say before) how much I like him? His behaviour to us has,\\n     in every respect, been as pleasing as when we were in Derbyshire.\\n     His understanding and opinions all please me; he wants nothing but\\n     a little more liveliness, and _that_, if he marry _prudently_, his\\n     wife may teach him. I thought him very sly; he hardly ever\\n     mentioned your name. But slyness seems the fashion. Pray forgive\\n     me, if I have been very presuming, or at least do not punish me so \\nMedata: {'file location': 'data', 'file type': 'txt', 'file title': 'PrideAndPrejudice'}\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createChunkDocInfo(chunk):\n",
    "    return chunk.page_content +' \\nMedata: '+ str(chunk.metadata)\n",
    "\n",
    "createChunkDocInfo(randomchunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a35e64d1-6b1c-4cf6-81d6-cbd68f02a266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getEmbeddings(model, listOfText):\n",
    "    return model.encode(listOfText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bac834cd-dead-4b9c-858b-bcf93ed897db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.79663170e-02  1.14518935e-02 -2.46319156e-02  7.18887374e-02\n",
      " -3.33716646e-02 -1.29302498e-02 -1.84203815e-02  5.69653362e-02\n",
      "  3.54221798e-02 -1.70748737e-02 -7.93525949e-03 -3.31766382e-02\n",
      "  3.05590569e-03 -5.00677414e-02 -5.41818514e-03  1.09316520e-02\n",
      "  2.73657981e-02  4.52942308e-03  4.29288223e-02 -6.39499724e-03\n",
      "  6.42575473e-02  3.71954404e-03  2.06486471e-02 -1.61153916e-02\n",
      "  2.64957491e-02  2.37572044e-02  1.81206397e-03  7.30221719e-02\n",
      "  7.53033301e-03 -1.93524174e-02  1.16033228e-02  9.86518897e-03\n",
      " -6.01424091e-02 -2.88560446e-02  2.28276440e-06 -7.92555790e-03\n",
      " -1.77070927e-02 -1.91813447e-02 -1.49291465e-02  1.20882848e-02\n",
      "  7.28819817e-02 -1.79723240e-02  2.43832693e-02 -7.76596228e-03\n",
      " -2.64713038e-02  1.28358090e-02 -1.74699295e-02  3.36028822e-02\n",
      "  3.23083140e-02  5.69985770e-02  1.19521702e-02 -2.66817063e-02\n",
      " -5.29866957e-04 -4.72773835e-02  1.27919778e-01 -2.15720888e-02\n",
      " -1.89321227e-02 -6.92544058e-02 -4.09197137e-02  4.42662574e-02\n",
      "  3.18731964e-02 -1.68058705e-02 -7.79390112e-02 -2.51191016e-02\n",
      " -5.79869561e-03  3.40560055e-03  1.47640973e-03  1.69068798e-02\n",
      " -2.47434191e-02 -5.64153977e-02  1.17873125e-01  6.07970497e-03\n",
      "  3.71042080e-02  3.92533429e-02 -5.67201860e-02  8.65132920e-03\n",
      "  1.48395905e-02 -6.44775555e-02 -2.79388241e-02 -3.24742794e-02\n",
      "  3.11782733e-02 -1.98866185e-02 -1.53261200e-02  2.84860637e-02\n",
      " -2.00632606e-02  3.25694047e-02 -1.11718634e-02  6.27117837e-03\n",
      " -1.82344560e-02 -5.60976155e-02 -2.87993308e-02 -4.22163606e-02\n",
      " -1.13217859e-02  3.87162203e-03  8.27007741e-02 -2.80761048e-02\n",
      " -2.20309198e-02 -9.06138569e-02  4.40187007e-02 -4.71080393e-02]\n"
     ]
    }
   ],
   "source": [
    "embeddings = getEmbeddings(STmodel, createChunkDocInfo(randomchunk))\n",
    "print(embeddings[:100]) #Only creating the first 100 values of the embeddings vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25f2bccf-bfab-4441-9d3d-809177ad5a83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing length of first element of embeddings\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6baa1-d17c-43c0-ad2e-59a77ef506cb",
   "metadata": {},
   "source": [
    "Each embedding is a vector that is 768 elements long. The first 100 elements look like the print out above.\\\n",
    "Now to create embeddings for our entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1811b7c3-02cc-4b0e-a325-b7cb84dc2f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating a list of strings using the chunks of lanchain documents\n",
    "sentences = [createChunkDocInfo(chunk) for chunk in chunks]\n",
    "\n",
    "#creating embeddings for the resulting sentences\n",
    "embeddings = getEmbeddings(STmodel,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6044790-cc6e-45f5-9828-e56c6542d639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5161, 768)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7514b8-fbb3-4ede-93b9-313a38383ce8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf4fe7-3616-4870-92f1-5eb6eab359ca",
   "metadata": {},
   "source": [
    "> [FAISS](https://ai.meta.com/tools/faiss/#:~:text=FAISS%20(Facebook%20AI%20Similarity%20Search,more%20scalable%20similarity%20search%20functions) (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves limitations of traditional query search engines that are optimized for hash-based searches, and provides more scalable similarity search functions.\\\n",
    "> *-- FAISS page*\n",
    "\n",
    "`FAISS` effectively can act like a local vector DB to help store all our embeddings and efficiently retrieve similar embeddings and consequently, semantically similar sentences\n",
    "\n",
    "An example on how to implement FAISS using  ü§ó  transformers can be found [here](https://huggingface.co/learn/nlp-course/chapter5/6?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d2e4223-27be-41dc-ae34-e1e4ee40236c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "from faiss import write_index, read_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ca367-90cd-448d-9663-62644d54f6cf",
   "metadata": {},
   "source": [
    "If you're running the indexing the first time, the `FAISS` model will have to be run and depending on how large the corpus is and how it has been chunked can take a while. However, on future runs, the index can be directly loaded from your local hard drive if we write it to our local machine.\n",
    "\n",
    "The following cell tries to load the `FAISS` index from your local hard drive, and failing to do so, runs the indexing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7b9bedb-6b2b-4ad0-b226-98065f916332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index\n",
      "Writing index to local machine.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    index = read_index(\"faiss_index/FAISS_Embeddings.index\")\n",
    "    print(\"FAISS index successfully loaded from local machine\")\n",
    "except:\n",
    "    res = faiss.StandardGpuResources()\n",
    "    \n",
    "    print(\"Creating FAISS index\")\n",
    "    # build a flat (CPU) index\n",
    "    index_flat = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    # make it into a gpu index\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "    \n",
    "    gpu_index_flat.add(embeddings)         # add vectors to the index\n",
    "\n",
    "    print(\"Writing index to local machine.\")\n",
    "    write_index(index_flat, \"faiss_index/large.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605adf2b-03f8-49aa-8799-88c07635fad3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sample Query\n",
    "We'll use the same function as before to generate embeddings for potential queries. Transformer models are optimized to work well with vectors, and our function leverages that. As a result we can encode multiple queries simultaneously, and even if we have a singular query we should enclose it in a list.\n",
    "\n",
    "Lets try to do a semantic search on a few sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "485c01d6-6190-46c9-8b70-44738b3ea302",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\"where does the story of frakenstein take place?\",\"where does Alice in Wonderland take place?\"]\n",
    "queriesEmb = getEmbeddings(STmodel,queries)\n",
    "\n",
    "queriesEmb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77239bab-05de-4794-a571-120d611cdd02",
   "metadata": {},
   "source": [
    "The search function in `FAISS` is a generic function that can be used to search for the nearest neighbors of a query vector in a given index. The function takes the following arguments:\n",
    "- query: The query vector.\n",
    "- k: The number of nearest neighbors to return.\n",
    "\n",
    "and returns:\n",
    "- distances: A pre-allocated buffer to store the distances to the nearest neighbors.\n",
    "- indexes: A pre-allocated buffer to store the labels of the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "60ecb40d-6d80-4caf-9f56-2b38b2087748",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Distance of neghbors to query:\n",
      " [[1.0184469  1.0542233  1.082648   1.0858334  1.090111  ]\n",
      " [0.67878747 0.7000439  0.81062174 0.9521235  0.96657526]]\n",
      "--------------------------------------------------\n",
      "Index of neghbors:\n",
      "[[1659 1836 1990 1604 1985]\n",
      " [ 179    0    1  178  107]]\n",
      "--------------------------------------------------\n",
      "Note: There were 2 queries\n",
      "['where does the story of frakenstein take place?', 'where does Alice in Wonderland take place?']\n"
     ]
    }
   ],
   "source": [
    "numNeighbors = 5                          \n",
    "Distances, Indexes = gpu_index_flat.search(x=queriesEmb, k=numNeighbors)  \n",
    "\n",
    "print('-'*50)\n",
    "print(f\"Distance of neghbors to query:\\n {Distances[:5]}\")      \n",
    "print('-'*50)             \n",
    "print(f\"Index of neghbors:\\n{Indexes[:5]}\")                   \n",
    " \n",
    "print('-'*50)\n",
    "print('Note: There were 2 queries')\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7eddd760-9058-4f27-9bb7-d7454e121369",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "I used my knowledge of this phase of spiritual pathology, and laid down\n",
      "a rule that she should not be present with Lucy or think of her illness\n",
      "more than was absolutely required. She assented readily, so readily that\n",
      "I saw again the hand of Nature fighting for life. Van Helsing and I were\n",
      "shown up t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: Dracula\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "c = chunks[idx]\n",
    "printChunkInfo(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "567f2fae-d098-424a-ad69-3342da01a112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "For the query:    'where does the story of frakenstein take place?':\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "Chunk Index: 1659\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "I am already far north of London, and as I walk in the streets of\n",
      "Petersburgh, I feel a cold northern breeze play upon my cheeks, which\n",
      "braces my nerves and fills me with delight. Do you understand this\n",
      "feeling? This breeze, which has travelled from the regions towards\n",
      "which I am advancing, gives me\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: Frankenstein\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 1836\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "I quitted my seat, and walked on, although the darkness and storm\n",
      "increased every minute, and the thunder burst with a terrific crash\n",
      "over my head. It was echoed from Sal√™ve, the Juras, and the Alps of\n",
      "Savoy; vivid flashes of lightning dazzled my eyes, illuminating the\n",
      "lake, making it appear like a \n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: Frankenstein\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 1990\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "‚ÄúShe arrived in safety at a town about twenty leagues from the cottage\n",
      "of De Lacey, when her attendant fell dangerously ill. Safie nursed her\n",
      "with the most devoted affection, but the poor girl died, and the\n",
      "Arabian was left alone, unacquainted with the language of the country\n",
      "and utterly ignorant of\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: Frankenstein\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 1604\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "_6 November._--It was late in the afternoon when the Professor and I\n",
      "took our way towards the east whence I knew Jonathan was coming. We did\n",
      "not go fast, though the way was steeply downhill, for we had to take\n",
      "heavy rugs and wraps with us; we dared not face the possibility of being\n",
      "left without warm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: Dracula\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 1985\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "‚ÄúThe government of France were greatly enraged at the escape of their\n",
      "victim and spared no pains to detect and punish his deliverer. The\n",
      "plot of Felix was quickly discovered, and De Lacey and Agatha were\n",
      "thrown into prison. The news reached Felix and roused him from his\n",
      "dream of pleasure. His blind \n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: Frankenstein\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "For the query:    'where does Alice in Wonderland take place?':\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "Chunk Index: 179\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "Lastly, she pictured to herself how this same little sister of hers\n",
      "would, in the after-time, be herself a grown woman; and how she would\n",
      "keep, through all her riper years, the simple and loving heart of her\n",
      "childhood: and how she would gather about her other little children,\n",
      "and make _their_ eyes b\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: AliceInWonderland\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 0\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "The Project Gutenberg eBook of Alice's Adventures in Wonderland\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gu\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: AliceInWonderland\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 1\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "Alice‚Äôs Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Contents\n",
      "\n",
      " CHAPTER I.     Down the Rabbit-Hole\n",
      " CHAPTER II.    The Pool of Tears\n",
      " CHAPTER III.   A Caucus-Race and a Long Tale\n",
      " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
      " CHAPTER V.     Advice from a Cater\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: AliceInWonderland\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 178\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "So she sat on, with closed eyes, and half believed herself in\n",
      "Wonderland, though she knew she had but to open them again, and all\n",
      "would change to dull reality‚Äîthe grass would be only rustling in the\n",
      "wind, and the pool rippling to the waving of the reeds‚Äîthe rattling\n",
      "teacups would change to tinkling \n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: AliceInWonderland\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Chunk Index: 107\n",
      "Printing chunk (First 300 chars and the metadata)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page Content:\n",
      "Once more she found herself in the long hall, and close to the little\n",
      "glass table. ‚ÄúNow, I‚Äôll manage better this time,‚Äù she said to herself,\n",
      "and began by taking the little golden key, and unlocking the door that\n",
      "led into the garden. Then she went to work nibbling at the mushroom\n",
      "(she had kept a piec\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MetaData:\n",
      "{\n",
      "  file location: data\n",
      "  file type: txt\n",
      "  file title: AliceInWonderland\n",
      "}\n",
      "\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for q_num,q_str in enumerate(queries):\n",
    "    print('*'*100)\n",
    "    print(f\"For the query:    '{q_str}':\")\n",
    "    print('*'*100)\n",
    "    print('\\n')\n",
    "    for idx in Indexes[:5][q_num]:\n",
    "        print(f\"Chunk Index: {idx}\")\n",
    "        printChunkInfo(chunks[idx])\n",
    "        print('\\n')\n",
    "        print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a0175-c0cb-47c3-b746-f2355a48382e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Looks like our semantic search is working reasonably well. Although clearly some errors are being made. For eg:\n",
    "- The first query was in regards to `Frankenstein`'s story, instead it pulled information from `Dracula`. \n",
    "- For the second query, although all queries are from `Alice in Wonderland` per the query request, the top chunk has more to do with metadata about the printing of the book that was part of the text file.\n",
    "\n",
    "The model is probably relying fairly heavily on the tile in the metadata. If the filenames were completely random, or named using some kind of hashing, it may struggle to find relevant chunks. \n",
    "\n",
    "We'll see whether the LLM is able to distinguish the relevance, between the chunks to see whether it realizes what are passages from the story and what isn't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0088e1-7157-43a5-a02f-f04b1cea0812",
   "metadata": {},
   "source": [
    "# LLM  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294f7d7-219b-4e25-9d55-92d21babca7b",
   "metadata": {},
   "source": [
    "I am going to use `OpenAI`s LLM models, but even there you have a few differen't choices. `Da-Vinci` can help keep your costs down. I am going to opt for the ChatGPT 3.5 model because it is more performant. One can even \n",
    "\n",
    "For my purposes, I am going to pass a query and a list of relevant docs for the model to create responses. For this step I'd rather have a more precise and succint model, while utitlizing a more descriptive and creative model to summarize the final answer. So even though I have the same end point (`gpt-3.5-turbo`), I create 2 seperate rulesets for the model.\n",
    "\n",
    "Both of them are instructed to answer based on only the information provided.\n",
    "\n",
    "Note: Until this step, every all work has been performed on the local hardware, and no costs were incurred. API calls to the `OpenAI` will result in cost based on the number of tokens passed as part of the API call. A valid API key will also be necessary which can be created at the [OpenAI platform website](https://platform.openai.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f45c35f4-7bf3-49cc-a805-759187944cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a2693e1a-259f-45d2-9d81-d460978ae1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load API Key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ['API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7852272e-4490-4b51-a4e5-d8e39cb5035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm_precise = ChatOpenAI(\n",
    "    temperature=0.0, \n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "chatllm_descriptive = ChatOpenAI(\n",
    "    temperature=0.7, \n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "\n",
    "def queryLLM(query, \n",
    "             excerpt, \n",
    "             llm=chatllm_precise):    \n",
    "    \"\"\"\n",
    "    Queries a large language model (LLM) to answer a question using an excerpt of text.\n",
    "\n",
    "    Args:\n",
    "    query: The question to be answered.\n",
    "    excerpt: The excerpt of text to use to answer the question.\n",
    "        llm: The ChatOpenAI object to use.\n",
    "\n",
    "    Returns:\n",
    "    The LLM's response to the query.\n",
    "    \"\"\"\n",
    "    assistantRules = (\"Answer the question using only the excerpt provided. Do not make up information. Be precise and succint. \"+\n",
    "    \"Do not include the question. Do not include any sort of introduction to your response. Do not include an introduction in your response. Do not mention the excerpt.\")\n",
    "    \n",
    "    msg = f\"Q: {query}\" + \\\n",
    "          f\" Excerpt: {excerpt}\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(role = 'assistant',\n",
    "                      content=assistantRules), \n",
    "        HumanMessage(content=msg)\n",
    "    ]\n",
    "    \n",
    "    return llm(messages)\n",
    "\n",
    "def summarizeReponsesLLM(query, \n",
    "                         responses, \n",
    "                         llm=chatllm_descriptive):\n",
    "    \"\"\"\n",
    "    Summarizes the given responses using the given LLM model.\n",
    "\n",
    "    Args:\n",
    "        query: The query that was asked.\n",
    "        responses: A list of response objects.\n",
    "        llm: The ChatOpenAI object to use.\n",
    "\n",
    "    Returns:\n",
    "        A summary of the given responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    concat_responses = ' '.join([response.content for response in responses])\n",
    "    \n",
    "    assistantRules = (\"Answer the question by summarizing the responses. Do not make up information.\"+\n",
    "    \"Do not include the question. Do not include any sort of introduction to your response. Do not include an introduction in your response. \" +\n",
    "    \"Do not mention excerpts that provide no information\") \n",
    "    \n",
    "    \n",
    "    msg = f\"Q: {query}\" + \\\n",
    "          f\" Responses: {concat_responses}\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(role = 'assistant',\n",
    "                      content=assistantRules), \n",
    "        HumanMessage(content=msg)\n",
    "    ]\n",
    "    \n",
    "    return llm(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856bd74-096f-4f8c-9ecb-2198a2f3e938",
   "metadata": {},
   "source": [
    "### Test API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "82e5c75e-b78d-4b00-8902-a4f3a1cd4877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The temperature today in Dallas is expected to be mainly in the 90s.')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryLLM(query = \"whats the temperature today in Dallas?\",\n",
    "         excerpt= \"A warm weekend is expected. Highs today will be mainly in the 90s with 80s on Sunday. \\\n",
    "         A pattern shift will occur next week resulting in slightly cooler high temps along with scattered showers and thunderstorms each day. Severe weather is not expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b180f1e4-9904-44d2-864f-e2df39b3876a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The excerpt does not provide any information about the temperature in Dallas.')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryLLM(query = \"whats the temperature today in Dallas?\",\n",
    "         excerpt= \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8d55c33c-2d1c-4922-9f6d-7826e0715ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The excerpt does not provide information about the temperature in Dallas.')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryLLM(query = \"whats the temperature today in Dallas?\",\n",
    "         excerpt= \"Dallas (/Ààd√¶l…ôs/) is a city in Texas and the most populous in the Dallas‚ÄìFort Worth metroplex, the fourth-largest metropolitan area in the United States at 7.5 million people.\\\n",
    "         It is the most populous city in and seat of Dallas County with portions extending into Collin, Denton, Kaufman, and Rockwall counties.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af8e67-f64e-46fb-9165-3a1f4de56140",
   "metadata": {},
   "source": [
    "Ok. Looks like our API call functions are working as intended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1345f87-88db-4dc5-8d36-6613c51ef6fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f99ecf29-4e40-40c6-b4df-02299cf3cc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where does the story of frakenstein take place?',\n",
       " 'where does Alice in Wonderland take place?']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6cd2f0e6-6319-42c6-952f-23417aa8476a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extractRelevantChunksByIndex(chunks, indexes):\n",
    "    extracted_chunks = []\n",
    "    for index in indexes:\n",
    "        chunkInfo = createChunkDocInfo(chunks[index])\n",
    "        extracted_chunks.append(chunkInfo)\n",
    "\n",
    "    return extracted_chunks\n",
    "\n",
    "\n",
    "def getMostRelevantChunks_perQuery( queries, \n",
    "                                    numNeighbors, \n",
    "                                    chunks,\n",
    "                                    faiss_index = gpu_index_flat,\n",
    "                                    sentenceTransformer = STmodel):\n",
    "    \n",
    "    queriesEmb = getEmbeddings(sentenceTransformer,queries)    \n",
    "    Distances, mostRelevantChunkIndexes_perQuery = faiss_index.search(queriesEmb,\n",
    "                                                                k = numNeighbors) \n",
    "        \n",
    "    mostRelevantChunks_perQuery = [extractRelevantChunksByIndex(chunks, indexes)\n",
    "                                   for indexes in mostRelevantChunkIndexes_perQuery]\n",
    "        \n",
    "    return queries, queriesEmb, mostRelevantChunks_perQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ede8f38f-af52-49e5-b7c7-147ac99f9b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def queryAndSummarize(queries, \n",
    "                      chunks = chunks,\n",
    "                      numNeighbors=5):\n",
    "    responses_perQuery = defaultdict(list)\n",
    "    Results = {}\n",
    "    \n",
    "    queries, queriesEmb, relevantChunks_perQuery = getMostRelevantChunks_perQuery(queries,\n",
    "                                                                                 numNeighbors = numNeighbors,\n",
    "                                                                                 chunks = chunks)\n",
    "\n",
    "    for query, relevantChunks in zip(queries, relevantChunks_perQuery):\n",
    "        for chunk in relevantChunks:\n",
    "            response = queryLLM(query, chunk)\n",
    "            responses_perQuery[query].append(response)\n",
    "\n",
    "        finalAnswer = summarizeReponsesLLM(query=query,\n",
    "                                        responses=responses_perQuery[query]).content\n",
    "\n",
    "        Results[query] = {\n",
    "            'Responses': responses_perQuery[query],\n",
    "            'Summary':   finalAnswer\n",
    "        }\n",
    "        \n",
    "    return Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "44287662-576a-4387-a3d8-896c187bd808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = queryAndSummarize(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e42f4b34-9934-48b0-8a70-feeb904f5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprintResponses(responses):\n",
    "    for k,v in responses.items():\n",
    "        print(f\"Q: {k}\")\n",
    "        print(f\"A: {v['Summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a137f30b-9f7d-43a3-945e-5db0dcc077da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which families did Romeo and Juliet belong to?\n",
      "A: Romeo belonged to the Montague family and Juliet belonged to the Capulet family.\n",
      "Q: Who were some of the victims of Dracula?\n",
      "A: The responses do not provide any information about the victims of Dracula.\n",
      "Q: Who were Alice's friends in Alice in Wonderland?\n",
      "A: Alice's friends in Alice in Wonderland include the ten soldiers, the ten courtiers, the ten royal children, the guests (including the White Rabbit), and the Knave of Hearts.\n"
     ]
    }
   ],
   "source": [
    "pprintResponses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b978a-d939-428d-b4b9-c74ff844684c",
   "metadata": {},
   "source": [
    "Lets try it with different questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6cf45028-aefc-4edf-a2ae-0180a2f35b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which families did Romeo and Juliet belong to?\n",
      "A: Romeo belonged to the Montague family and Juliet belonged to the Capulet family.\n",
      "Q: Who were some of the victims of Dracula?\n",
      "A: The responses do not provide any information about the victims of Dracula.\n",
      "Q: Who were Alice's friends in Alice in Wonderland?\n",
      "A: Alice's friends in Alice in Wonderland include the ten soldiers, the ten courtiers, the ten royal children, the guests (including the White Rabbit), and the Knave of Hearts.\n"
     ]
    }
   ],
   "source": [
    "queries=[\n",
    "    \"Which families did Romeo and Juliet belong to?\",\n",
    "    \"Who were some of the victims of Dracula?\",\n",
    "    \"Who were Alice's friends in Alice in Wonderland?\"\n",
    "]\n",
    "\n",
    "\n",
    "responses = queryAndSummarize(queries,numNeighbors=10)\n",
    "\n",
    "pprintResponses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67215268-eafc-4d2b-a72a-16ca5808e57f",
   "metadata": {},
   "source": [
    "Although this results in reasonably good answers, but it clearly has limitations. \n",
    "\n",
    "Our embedding semantic search may not be able to help identify who Alice's friends are, the Cheshire Cat and the Mad Hatter don't make an appearance, but the Knave of Hearts who is an enemy does.\n",
    "\n",
    "Similarly, our Dracula victims couldn't be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5da0cca0-4bdf-4c9d-b463-bf8a2538928d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Describe the castle of Dracula.\n",
      "A: The Castle Dracula is located in the Carpathian mountains and is described as being on the edge of a precipice with a view of green treetops and deep rifts. It is described as having doors everywhere that are locked and bolted, with the only available exits being the windows. The castle is old and big, with broken walls and cold wind. The narrator explores various stairs, passages, and doors, finding one that is not locked. The castle is situated deep under a hill and can be seen in all its grandeur from a distance. The Count of Dracula is seen crawling down the castle wall with his cloak spread out like wings. The narrator also sees what appears to be the evil face of Count Dracula in the shadows of a dark passage. The landlord receives a letter from Count Dracula instructing him to secure the best place on the coach for the narrator, but refuses to provide details or speak about Count Dracula and his castle.\n",
      "Q: Why did Dorian Gray want his portrait made?\n",
      "A: Dorian Gray wanted his portrait made because he was curious about Basil Hallward's refusal to exhibit it, he wanted to capture and preserve his own beauty, he believed the portrait would absorb his whole nature, soul, and art, and he wanted to hide a secret from Basil.\n"
     ]
    }
   ],
   "source": [
    "queries=[\n",
    "    \"Describe the castle of Dracula.\",\n",
    "    \"Why did Dorian Gray want his portrait made?\",\n",
    "]\n",
    "\n",
    "\n",
    "responses = queryAndSummarize(queries,numNeighbors=10)\n",
    "\n",
    "pprintResponses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4de4a-27aa-45f0-9199-a62c5eda621d",
   "metadata": {},
   "source": [
    "# References and Citations:\n",
    "- [Understanding Neural Network Embeddings](https://towardsdatascience.com/understanding-neural-network-embeddings-851e94bc53d2) - Frank Liu\n",
    "- [FAISS](https://github.com/facebookresearch/faiss/)\n",
    "- [Hugging Face](https://huggingface.co/)\n",
    "    - [Sentence Transformers](https://huggingface.co/sentence-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714269b-1013-4a3f-91d6-fc2aa5f4a70a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
